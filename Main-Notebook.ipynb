{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import folium\n",
    "import string\n",
    "from helpers import *\n",
    "\n",
    "ALPHA = string.ascii_letters\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's on my plate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, our initial research questions are:\n",
    "\n",
    "- What is the proportion of imported/exported products per country?\n",
    "- Which countries sell the highest variety of eco-friendly products in terms of packaging and CO2 footprint?\n",
    "- Which countries have the largest variety of organic-labeled products?\n",
    "- What is the average nutrition score of every product category?\n",
    "- What ingredients imply a better nutrition score (or vice-versa)?\n",
    "\n",
    "From those in mind, we will analyse the dataset in order to see which features would be usefull to answer them and and most of all find if there is enough data to answer them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CSV file\n",
    "The CSV File is stored on our computer with the name `OpenFood.csv`. From opening it with a text editor, we found out that the cells are separated by tabulations (i.e. by the `\\t` character). Even though the file size is approximately 1.7 GB, we still manage to fit it in a dataframe. We start by opening it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mathias/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,3,5,24,25,26,28,36,41,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "path = \"\"\n",
    "file_name = \"OpenFood.csv\"\n",
    "data_path = path+file_name\n",
    "df = pd.read_csv(data_path, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>url</th>\n",
       "      <th>creator</th>\n",
       "      <th>created_t</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>last_modified_t</th>\n",
       "      <th>last_modified_datetime</th>\n",
       "      <th>product_name</th>\n",
       "      <th>generic_name</th>\n",
       "      <th>quantity</th>\n",
       "      <th>...</th>\n",
       "      <th>carbon-footprint_100g</th>\n",
       "      <th>nutrition-score-fr_100g</th>\n",
       "      <th>nutrition-score-uk_100g</th>\n",
       "      <th>glycemic-index_100g</th>\n",
       "      <th>water-hardness_100g</th>\n",
       "      <th>choline_100g</th>\n",
       "      <th>phylloquinone_100g</th>\n",
       "      <th>beta-glucan_100g</th>\n",
       "      <th>inositol_100g</th>\n",
       "      <th>carnitine_100g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000000017</td>\n",
       "      <td>http://world-en.openfoodfacts.org/product/0000...</td>\n",
       "      <td>kiliweb</td>\n",
       "      <td>1529059080</td>\n",
       "      <td>2018-06-15T10:38:00Z</td>\n",
       "      <td>1529059204</td>\n",
       "      <td>2018-06-15T10:40:04Z</td>\n",
       "      <td>Vitória crackers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000000000031</td>\n",
       "      <td>http://world-en.openfoodfacts.org/product/0000...</td>\n",
       "      <td>isagoofy</td>\n",
       "      <td>1539464774</td>\n",
       "      <td>2018-10-13T21:06:14Z</td>\n",
       "      <td>1539464817</td>\n",
       "      <td>2018-10-13T21:06:57Z</td>\n",
       "      <td>Cacao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130 g</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000000000123</td>\n",
       "      <td>http://world-en.openfoodfacts.org/product/0000...</td>\n",
       "      <td>kiliweb</td>\n",
       "      <td>1535737982</td>\n",
       "      <td>2018-08-31T17:53:02Z</td>\n",
       "      <td>1535737986</td>\n",
       "      <td>2018-08-31T17:53:06Z</td>\n",
       "      <td>Sauce Sweety chili 0%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000000000291</td>\n",
       "      <td>http://world-en.openfoodfacts.org/product/0000...</td>\n",
       "      <td>kiliweb</td>\n",
       "      <td>1534239669</td>\n",
       "      <td>2018-08-14T09:41:09Z</td>\n",
       "      <td>1534239732</td>\n",
       "      <td>2018-08-14T09:42:12Z</td>\n",
       "      <td>Mendiants</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000000000949</td>\n",
       "      <td>http://world-en.openfoodfacts.org/product/0000...</td>\n",
       "      <td>kiliweb</td>\n",
       "      <td>1523440813</td>\n",
       "      <td>2018-04-11T10:00:13Z</td>\n",
       "      <td>1523440823</td>\n",
       "      <td>2018-04-11T10:00:23Z</td>\n",
       "      <td>Salade de carottes râpées</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            code                                                url   creator  \\\n",
       "0  0000000000017  http://world-en.openfoodfacts.org/product/0000...   kiliweb   \n",
       "1  0000000000031  http://world-en.openfoodfacts.org/product/0000...  isagoofy   \n",
       "2  0000000000123  http://world-en.openfoodfacts.org/product/0000...   kiliweb   \n",
       "3  0000000000291  http://world-en.openfoodfacts.org/product/0000...   kiliweb   \n",
       "4  0000000000949  http://world-en.openfoodfacts.org/product/0000...   kiliweb   \n",
       "\n",
       "    created_t      created_datetime last_modified_t last_modified_datetime  \\\n",
       "0  1529059080  2018-06-15T10:38:00Z      1529059204   2018-06-15T10:40:04Z   \n",
       "1  1539464774  2018-10-13T21:06:14Z      1539464817   2018-10-13T21:06:57Z   \n",
       "2  1535737982  2018-08-31T17:53:02Z      1535737986   2018-08-31T17:53:06Z   \n",
       "3  1534239669  2018-08-14T09:41:09Z      1534239732   2018-08-14T09:42:12Z   \n",
       "4  1523440813  2018-04-11T10:00:13Z      1523440823   2018-04-11T10:00:23Z   \n",
       "\n",
       "                product_name generic_name quantity      ...        \\\n",
       "0           Vitória crackers          NaN      NaN      ...         \n",
       "1                      Cacao          NaN    130 g      ...         \n",
       "2      Sauce Sweety chili 0%          NaN      NaN      ...         \n",
       "3                  Mendiants          NaN      NaN      ...         \n",
       "4  Salade de carottes râpées          NaN      NaN      ...         \n",
       "\n",
       "  carbon-footprint_100g nutrition-score-fr_100g nutrition-score-uk_100g  \\\n",
       "0                   NaN                     NaN                     NaN   \n",
       "1                   NaN                     NaN                     NaN   \n",
       "2                   NaN                     NaN                     NaN   \n",
       "3                   NaN                     NaN                     NaN   \n",
       "4                   NaN                     NaN                     NaN   \n",
       "\n",
       "  glycemic-index_100g water-hardness_100g choline_100g phylloquinone_100g  \\\n",
       "0                 NaN                 NaN          NaN                NaN   \n",
       "1                 NaN                 NaN          NaN                NaN   \n",
       "2                 NaN                 NaN          NaN                NaN   \n",
       "3                 NaN                 NaN          NaN                NaN   \n",
       "4                 NaN                 NaN          NaN                NaN   \n",
       "\n",
       "  beta-glucan_100g inositol_100g carnitine_100g  \n",
       "0              NaN           NaN            NaN  \n",
       "1              NaN           NaN            NaN  \n",
       "2              NaN           NaN            NaN  \n",
       "3              NaN           NaN            NaN  \n",
       "4              NaN           NaN            NaN  \n",
       "\n",
       "[5 rows x 173 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this first look at the DataFrame we can immediately remove the following columns as they are useless for our research purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'url',\n",
    "    'creator',\n",
    "    'created_t',\n",
    "    'created_datetime',\n",
    "    'last_modified_t',\n",
    "    'last_modified_datetime',\n",
    "    'generic_name',\n",
    "    'quantity',\n",
    "    'emb_codes',\n",
    "    'emb_codes_tags',\n",
    "    'purchase_places',\n",
    "    'stores',\n",
    "    'traces',\n",
    "    'traces_tags',\n",
    "    'serving_size',\n",
    "    'no_nutriments',\n",
    "    'image_url',\n",
    "    'allergens', \n",
    "    'image_small_url',\n",
    "    'manufacturing_places_tags',\n",
    "    'image_ingredients_url',\n",
    "    'image_ingredients_small_url',\n",
    "    'image_nutrition_url',\n",
    "    'image_nutrition_small_url',\n",
    "    'states',        \n",
    "    'states_en',\n",
    "    'states_tags',\n",
    "    'serving_quantity',\n",
    "    'traces_en',\n",
    "    'allergens_en',\n",
    "    'ingredients_that_may_be_from_palm_oil',\n",
    "    'ingredients_from_palm_oil'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the colums and put the result in a new data frame called df2\n",
    "df2 = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to get an idea of what the values are as there are loads of NaN values... To get a better idea of the values from the dataframe, we show the unique values of each columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We create a new dataframe\n",
    "df_unique = pd.DataFrame()\n",
    "\n",
    "# For each column\n",
    "for col in df2.columns:\n",
    "    # We take only the unique values of every column\n",
    "    df_unique = pd.concat([df_unique, pd.Series(df2[col].unique())], ignore_index=True, axis=1, copy=False)\n",
    "df_unique.columns= df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a large number of NaN values we also show the number of non NaN values of each column along with the number of different values in each of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We initialize the columns to be used in the table that will be shown\n",
    "cols = ['Number of non NaN values per column', 'Number of different values in each columns']\n",
    "\n",
    "# We create a table to show the informations\n",
    "describe_df = pd.DataFrame([df2.count(),df_unique.count()]).transpose()\n",
    "describe_df.columns = cols\n",
    "describe_df['ColNumber'] = np.arange(df_unique.shape[1])\n",
    "describe_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After further looking into the dataset, we see that there are some columns that are redundant such as `packaging` and `packaging_tags` so we keep only one of the two. For this specific example we only keep the `packaging_tags` column as the values are more consistent. We also remove some other columns. The reasons for the removal are stated as comment in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_2 = [\n",
    "    'packaging', # The values are redundant and be used in packaging_tags\n",
    "    'brands', # brands_tags has more stable (same rows are represented in different ways) values\n",
    "    'categories', # categories_tags has more stable values\n",
    "    'categories_en', # categories_tags has more stable values\n",
    "    'origins', # origins_tags has more stable values\n",
    "    'labels', # labels_tags has more stable values\n",
    "    'labels_en', # labels_tags has more stable values\n",
    "    'first_packaging_code_geo', # Not interesting for our research questions\n",
    "    'cities', # Contains only NaN values and not really intresting for our research questions\n",
    "    'cities_tags', # Contains only NaN values and not really intresting for our research questions\n",
    "    'countries', # Seem to contain useless duplicated values (France, en:FR, en:france,...)\n",
    "    'countries_en', # The countries_tags column gives a more stable result\n",
    "    'ingredients_text', # too hard to deal with and not really intresting after seeing the values\n",
    "    'additives', # Not needed for our research question, only the number of additives can be interesting\n",
    "    'additives_en', # Not needed for our research question, only the number of additives can be interesting\n",
    "    'ingredients_that_may_be_from_palm_oil_n', # As we already keep ingredients_from_palm_oil_n\n",
    "    'ingredients_that_may_be_from_palm_oil_tags', # As we already keep ingredients_from_palm_oil_n\n",
    "    'nutrition_grade_fr', # Contains only NaN values\n",
    "    'ingredients_from_palm_oil_tags', # Not useful for our research\n",
    "    'ingredients_from_palm_oil_n', # Not useful as well\n",
    "    'main_category_en' # The main_category column has more stable values\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the nutrition facts we can delete all the columns that have less than 1000 values (count) because if they do, it means that the columns are too specific (like type of sugar) or there aren't enough values in the dataset in the columns to be interessting. Note this means we have to drop the idea of analyzing the carbon footprint as there aren't enough values to answer our carbon foot print analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the list of all the food fact (ingredients) column to delete since we do not need them for our research\n",
    "describe_df_aux = describe_df.iloc[36:]\n",
    "\n",
    "# We set them as a list for the removal\n",
    "food_facts_to_drop = list(describe_df_aux.index.values)\n",
    "\n",
    "# We remove the nutrition score from the list since they are needed\n",
    "food_facts_to_drop.remove('nutrition-score-fr_100g')\n",
    "food_facts_to_drop.remove('nutrition-score-uk_100g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the two lists we can merge them and then drop the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two list created above to remove them\n",
    "columns_to_drop_merged = columns_to_drop_2 + food_facts_to_drop\n",
    "# Deleting and storing in a new dataframe\n",
    "df3 = df2.drop(columns_to_drop_merged, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can wonder if the `code` column can be set as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.code.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the code column is not unique let's see how many values are duplicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take all the rows where the code is duplicates\n",
    "duplicate_codes_indices = np.where(df3.duplicated('code', keep=False).values)[0]\n",
    "dups = df3.iloc[duplicate_codes_indices].copy()\n",
    "dups.code = dups.code.astype(float)\n",
    "\n",
    "# We sort so that the duplicated rows are on top of each other\n",
    "dups.sort_values(by = 'code', inplace = True)\n",
    "\n",
    "# Show the result\n",
    "dups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen there are two types of duplicates:\n",
    "Some codes (`code` column) appear twice and some codes are NaN. As it can be observed when codes are duplicated, they are often the same product and sometimes one of the rows contains more information than the other for the same code. So we could just keep the rows that contain the most information. And for the rows with NaN as code, the values of the columns are not matching the column names so there must be a shift of values. As a lot of values are missing, it would be complex to recover the true data and there are only 34 rows with code equal to NaN, so we can delete them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = dups[dups != float('nan')]\n",
    "\n",
    "dups['nb_of_features'] = np.sum(dups.notnull(), axis=1)\n",
    "dups.reset_index(inplace = True)\n",
    "indices_to_remove = dups.sort_values(['code','nb_of_features'],ascending = True)\\\n",
    "                        .groupby('code',as_index=False).first()['index'].values\n",
    "df3.dropna(subset=['code'],inplace=True)\n",
    "df3.drop(indices_to_remove,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the `code` column of the dataframe is unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3.code.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we just have to set this column as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.set_index('code', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has the following shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0}(row) x {1}(columns)'.format(df3.shape[0], df3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are using these proportions of the initial dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:1.2f}% of the rows of the initial dataframe'.format(df3.shape[0]/df.shape[0]*100))\n",
    "print('{0:1.2f}% of the columns of the initial dataframe'.format(df3.shape[1]/df.shape[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types and statistics of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check what are the type of the columns of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the types are object as there are a lot of NaN values (which are float type) in the text so the python compiler can put string as type. We will be able to replace some of the NaN values later.\n",
    "\n",
    "Let's plot the percentage of the non NaN values in each general information features i.e. column 0 to 18. The data set is split with respect to some groups of columns to have make a better representation of each category of feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_info = df3.iloc[:,:18]\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.bar(range(general_info.shape[1]),general_info.count()/general_info.shape[0]*100)\n",
    "plt.xticks(range(general_info.shape[1]), general_info.columns,rotation=70)\n",
    "plt.title(\"Percentage of non-NaN values for each feature\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.ylabel(\"Percentage of non-NaN values [%]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from looking at the data frame, there is a large number of NaN values. Indeed, for the `origin_tags` there is more than 95% of the values that are NaN values. This is compromising a lot with our research question about import/export. On the other hand, from the `df_unique` dataframe we see that there is still ~50'000 rows that contain an origin tags so we will still be able to do some research (though not as extended as we wished) on a subset of rows.\n",
    "\n",
    "We also plot an histogram of the count of non-NaN values for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(121)\n",
    "plt.hist(general_info.transpose().count(),\n",
    "         bins = np.arange(general_info.shape[1]+2)-0.5,\n",
    "         rwidth = 0.8)\n",
    "plt.xticks(range(general_info.shape[1]+1))\n",
    "plt.title(\"Histogram of the number of non-NaN values\")\n",
    "plt.xlabel(\"Number of non-NaN values\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(general_info.transpose().count(),\n",
    "         bins = np.arange(general_info.shape[1]+2)-0.5, \n",
    "         cumulative=True,\n",
    "         rwidth = 0.8)\n",
    "plt.xticks(range(general_info.shape[1]+1))\n",
    "plt.title(\"Cumulative histogram of the number of non-NaN values\")\n",
    "plt.xlabel(\"Number of non-NaN values\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing the problem row-wise, we can evaluate the amount of data that we will be able to use. We can see the distribution on the left to get a general idea of the number of non-NaN values for each row. But the most interesting plot is the cumulative on the right. We can see that more than 150'000 rows have more than 9 non-NaN values. So we will still be able to play with a lot of data eventhough we will have to drop loads of rows. We still have to figure out what threshold. This is depending on what specific aspect is studied!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing a second data set with all the countries locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV found on this page: https://developers.google.com/public-data/docs/canonical/countries_csv\n",
    "countries_df = pd.read_csv(\"countries.csv\", sep = '\\t')\n",
    "\n",
    "# Remove the space at the end of the column names\n",
    "countries_df.columns = ['country', 'latitude', 'longitude', 'name']\n",
    "\n",
    "# Setting country as Index\n",
    "countries_df.set_index('country', inplace = True)\n",
    "countries_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this second data set we will be able to see how the products travel across the world. To be more specific, we wil be able to compute the distance traveled from the countries coordinates on the map (by computing the grand circle distance). As we all know, the fact of importing food that comes from far away is bad for the environment. Mainly for vegetable as they have to travel most of the time by plane otherwise they get quickly out of date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More analysis of the research questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the proportion of imported/exported products per country?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both `origins_tags` and `countries_tags`, we can extract the countries that import and export the most number of products. We can assume that the name of the product are unique and therefore can be used to look for the number of products sold in or are being sold by a particular country. The countries and origins tags are not cleaned yet, but it is definitely possible to search whether it might countain the name of a known country or the abbreviation of it to do or from one of its cities. We also plan to extract the known coordinate for every country and map them to the origins_tags and countries_tags and display the flow of products inward or outward from every single country and its volume on a world map to know what country produces the most of the products used worldwide or the one that imports the most of its products. With the second data frame we imported, we will be able to show the distance traveled by the product (using the great distant circle) in the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of unique values for origins_tags\n",
    "len(df3[\"origins_tags\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of unique values for countries\n",
    "len(df3[\"countries_tags\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of products sold in a specific country\n",
    "df3.copy().groupby(by = ['countries_tags'])[\"product_name\"].count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of products made in a specific country\n",
    "df3.copy().groupby(by = ['origins_tags'])[\"product_name\"].count().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the aggregation by `countries_tags` has 6681 possible unique values (excluding NaN) which is much more than the possible number of countries in the world. After exploring the countries_tags, we observe the some products are sold in more than a single country, and the combination of all the product in which it is sold is considered a single entry for `origins_tags`. Thus, a seperation of the `countries_tags` is neccessary and reaggregation of the all the products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice that origins_tags has 1237 unique values (excluding NaN) whici is more than the possible number of countries in the world. After exploring origins_tags, we observe the some origin_tags are more specific about the origins than just including the country. We should clean this column by looking at whether it contains a known name of a country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which countries sell the highest variety of eco-friendly products in terms of packaging and CO2 footprint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained earlier, the column `carbon-footprint_100g` has less than 1000 values. This jeopardizes this research question on one hand since CO2 footprint is a vital aspect and the data is lacking. As a result, we will not be using the CO2 footprint in this research question since we have decided to remove the column. On the other hand, we will be able to move forward by analyzing the `packaging_tags` column. <br>\n",
    "Let's see if the `packaging_tags` and `countries_tags` column have enough data for this question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['packaging_tags'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['countries_tags'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the data relative to the packaging has more than 140,000 non-NaN values and the data relative to the country where is it ***sold*** is present in nearly all products. In this sense, by using the information of the packaging along with the column `countries_tags`, we can estimate the countries that sell the highest variety of eco-friendly products but only in terms of packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which countries have the largest variety of organic-labeled products? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This research question appeals to three different columns: `countries_tags`, `categories_tags` and `labels_tags`. The labels and categories have data about wether the products are organic or not, while the countries will be used to identify which of the countries have the largest variety of organic-labeled products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3['categories_tags'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have data of the categories for more than 180,000 products. As a result, this column has enough data to be considered for this research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['labels_tags'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice that we have data of the labels for more than 100,000 products. As a result, this column has enough data to be considered for this research question. <br>\n",
    "Since we know that the `countries_tags` column is filled for almost every product, we can conclude that this research question can be answered by looking at the organic labels in `labels_tags` and possibly `categories_tags` and mapping the labels to the countries from `countries_tags`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average nutrition score of every product category? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer this research question, we will be diving into the `nutrition-score-fr_100g`, `nutrition-score-uk_100g` as well as some columns about the categories: `categories_tags`, `main_category`, `pnns_groups_1` and `pnns_groups_2`. Since we have already seen the number of non-NaN values for `categories_tags` above, we can now analyze the other columns that can be very helpful for this research question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['nutrition-score-fr_100g'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['nutrition-score-uk_100g'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that `nutrition-score-uk_100g` and `nutrition-score-fr_100g` have the exact same number of the non-NaN values. We can check how similar they are columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['nutrition-score-uk_100g'].corr(df3['nutrition-score-fr_100g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two columns are 93% similar. We can then move on with only one of the columns since they are nearly exactly the same. <br> Let's analyze further the columns related to the product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df3['main_category'].count())\n",
    "print(df3['pnns_groups_1'].count())\n",
    "print(df3['pnns_groups_2'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that for every column that is related to the category of the product, we do indeed have enough data (at least 180,000 products). By mapping the categories to the nutrition score, we can find out the average nutrition score for every category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering the research questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first change the name of our dataframe for better readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to some more cleaning for columns that will be relevant later, such as `countries_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data['countries_tags'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the name of the countries comes after the :, we can then take the substring starting from \":\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a copy of the column\n",
    "countries = current_data['countries_tags'].astype(str)\n",
    "\n",
    "# We split and take the second element, for example en:france would return france\n",
    "countries = countries.str.split(':').str.get(1)\n",
    "\n",
    "# We capitalize the first letter\n",
    "countries = countries.str.capitalize()\n",
    "\n",
    "# Some results are United-states, we remove the \"-\" for better readability\n",
    "countries = countries.str.replace(\"-\", \" \")\n",
    "\n",
    "# We check the result\n",
    "countries.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the result is satifsying, we still notice that some countries have their name followed by either \",en\", \",fr\", \",de\", \",ch\", \",it\", \",sq\" or \",es\". We can remove them to only keep the country. Moreover, some country names are \"To be checked\", however we should not remove them because those rows may contain data that is relevant to other research questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the names that contain extra characters that are not needed. Since they are not a lot, \n",
    "# we can do it manually.\n",
    "countries = countries.str.replace(\",en\", \"\")\n",
    "countries = countries.str.replace(\",fr\", \"\")\n",
    "countries = countries.str.replace(\",de\", \"\")\n",
    "countries = countries.str.replace(\",ch\", \"\")\n",
    "countries = countries.str.replace(\",it\", \"\")\n",
    "countries = countries.str.replace(\",sq\", \"\")\n",
    "countries = countries.str.replace(\",es\", \"\")\n",
    "\n",
    "# We also replace the values representing the same country to the country that is represented\n",
    "countries = countries.str.replace(\"Suisse\", \"Switzerland\")\n",
    "\n",
    "countries = countries.str.replace(\"French polynesia\", \"France\")\n",
    "countries = countries.str.replace(\"French guiana\", \"France\")\n",
    "countries = countries.str.replace(\"Martinique\", \"France\")\n",
    "countries = countries.str.replace(\"Guadeloupe\", \"France\")\n",
    "countries = countries.str.replace(\"New caledonia\", \"France\")\n",
    "countries = countries.str.replace(\"Reunion\", \"France\")\n",
    "countries = countries.str.replace(\"Frankreich\", \"France\")\n",
    "countries = countries.str.replace(\"Frankrike\", \"France\")\n",
    "\n",
    "countries = countries.str.replace(\"Deutschland\", \"Germany\")\n",
    "countries = countries.str.replace(\"Allemagne\", \"Germany\")\n",
    "\n",
    "# We put countries into a dataframe\n",
    "countries_in_df = countries.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the column is cleaned, we can set it as the column of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data['countries_tags'] = countries_in_df['countries_tags']\n",
    "current_data['countries_tags'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the proportion of imported/exported products per country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the fucntions used in this cell are stored in the helpers.py file at root level of the git repository\n",
    "# They were placed here as their number of lines sum up to 250 line of code\n",
    "\n",
    "# Furhter cleaning of the countries tags\n",
    "ct = clean_coutries_tags(current_data)\n",
    "current_data['countries_tags'] = ct\n",
    "\n",
    "ot = clean_origins_tags(current_data)\n",
    "current_data['origins_tags'] = ot\n",
    "\n",
    "mt = clean_manufact(current_data)\n",
    "current_data['manufacturing_places'] = mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_tags = current_data[['origins_tags','product_name']].groupby('origins_tags').count().sort_values(by= 'product_name', ascending=False).iloc[1:]\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.barh(ori_tags.head(20).index, ori_tags.head(20).product_name, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ctry_tags = current_data[['countries_tags','product_name']].groupby('countries_tags').count().sort_values(by= 'product_name', ascending=False)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.barh(ctry_tags.head(20).index, ctry_tags.head(20).product_name, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "manf_plce = current_data[['manufacturing_places','product_name']].groupby('manufacturing_places').count().sort_values(by= 'product_name', ascending=False).iloc[1:]\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.barh(manf_plce.head(20).index, manf_plce.head(20).product_name, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp_exp = current_data[current_data['origins_tags'] != 'nan']\n",
    "imp_exp = imp_exp[['countries_tags','origins_tags','product_name']].groupby(['countries_tags','origins_tags'])\\\n",
    "            .count()\\\n",
    "            .sort_values(by='product_name', ascending=False)\n",
    "imp_exp.loc['France']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp_exp.loc['France'].iloc[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data[['origins_tags', 'product_name']].groupby('origins_tags').count().sort_values(by='product_name', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which countries contribute most to the environment through eco-friendly packages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer this question, it is important to do some research to understand which packaging are the most/least eco-friendly. Online websites such as https://bizfluent.com/facts-6743400-eco-friendly-packaging-.html gave us some insights. First, we noticed that the \n",
    "Our research research has led to following conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first take the rows with a non-null value in packaging_tags\n",
    "packaging_list_not_null = current_data[current_data['packaging_tags'].notnull()]\n",
    "\n",
    "# We then take only the columns we are interested in: packaging_tags and countries_tags\n",
    "packaging_list = packaging_list_not_null[['countries_tags','packaging_tags']]\n",
    "\n",
    "# We put packaging_tags in a list, separate them by the \",\" which separates the different packaging\n",
    "packaging_list['packaging_tags'] = packaging_list['packaging_tags'].str.split(',')\n",
    "\n",
    "# We set the countries_tags as index to \"explode\" the list into different rows\n",
    "packaging_list = packaging_list.set_index('countries_tags')\n",
    " \n",
    "# We do the \"explode\"\n",
    "packaging_list = (packaging_list.packaging_tags.apply(pd.Series)\n",
    "              .stack()\n",
    "              .reset_index(level=1, drop=True)\n",
    "              .to_frame('packaging'))\n",
    "\n",
    "# We reset the index\n",
    "packaging_list = packaging_list.reset_index(level=0)\n",
    "\n",
    "packaging_list = packaging_list.reset_index(level=0)\n",
    "\n",
    "# Show the output\n",
    "packaging_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have for each country what packaging it uses for each product, we need some way to quantify it. Thus, we create a dictionary containing for each packaging a certain grade, where the higher the grade the more it is eco-friendly. We can then group by the country and take the mean (dividing by the number of products)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the grades for each packaging type\n",
    "# Note that we take the most common packaging types, where some appear in both french and english\n",
    "# The highest the score, the more the packaging is eco-friendly\n",
    "packaging_to_grade = {\n",
    "    'verre':50,\n",
    "    'glass':50,\n",
    "    'carton':40,\n",
    "    'cardboard':40,\n",
    "    'papier':30,\n",
    "    'metal':20,\n",
    "    'aluminium':10,\n",
    "    'plastique': 5,\n",
    "    'plastic': 5\n",
    "}\n",
    "\n",
    "# We create a dataframe from the dicionary\n",
    "packaging_grade_df = pd.DataFrame.from_dict(packaging_to_grade, orient='index')\n",
    "packaging_grade_df = packaging_grade_df.reset_index()\n",
    "packaging_grade_df = packaging_grade_df.rename(columns={\"index\": \"packaging\", 0: \"grade\"})\n",
    "\n",
    "# We then merge on the packaging type in order to have a grade for each packaging and each country\n",
    "packaging_with_grade = packaging_list.merge(packaging_grade_df, how='left', left_on='packaging', right_on='packaging')\n",
    "\n",
    "# We take only those with non-NaN grade (since we took only the most common packaging types, NaN may appear)\n",
    "packaging_with_grade = packaging_with_grade[packaging_with_grade['grade'].notnull()]\n",
    "\n",
    "# We noticed that some countries only have around 1 or 2 products which is not enough data\n",
    "# We thus need to take into account only those that have a certain number of products, for example 10 products\n",
    "# We duplicate the grade column and call it count_products\n",
    "packaging_with_grade['count_products'] = packaging_with_grade['grade']\n",
    "\n",
    "# We then group by the countries, and aggregate the obtain the mean for each country and the number of products\n",
    "packaging_with_grade_count = packaging_with_grade.groupby('countries_tags').agg({'grade':'mean', 'count_products':'count'})\n",
    "\n",
    "# We then take only those with more than 10 products\n",
    "final_packaging_grade = packaging_with_grade_count[packaging_with_grade_count['count_products'] >= 10]\n",
    "\n",
    "# We sort the values\n",
    "final_packaging_grade = final_packaging_grade.sort_values(by='grade', ascending=False)\n",
    "final_packaging_grade = final_packaging_grade.reset_index(level=0)\n",
    "\n",
    "# We take the top 10 and plot it\n",
    "final_packaging_grade_top_10 = final_packaging_grade.head(10)\n",
    "final_packaging_grade_top_10.plot.bar(x='countries_tags', y='grade', title='Top 10 eco-friendly countries')\n",
    "\n",
    "# We take the bottom 10 and plot it\n",
    "final_packaging_grade_bottom_10 = final_packaging_grade.tail(10)\n",
    "final_packaging_grade_bottom_10.plot.bar(x='countries_tags', y='grade', title='Top 10 non eco-friendly countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which countries have the largest variety of organic-labeled products? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We take only the rows that do not have a null label\n",
    "labels_not_null = current_data[current_data['labels_tags'].notnull()]\n",
    "categories_not_null = current_data[current_data['categories_tags'].notnull()]\n",
    "\n",
    "# We take the rows with the label containing \"organic\" or \"green\" or \"bio\"\n",
    "labels_not_null_organic = labels_not_null[labels_not_null['labels_tags'].str.contains('organic|green|bio')]\n",
    "\n",
    "# We take the countries that have one of those labels\n",
    "countries_with_labels_organic = labels_not_null_organic[['labels_tags', 'countries_tags']]\n",
    "\n",
    "# We also take the number of products per country that just have a non-null label to compute a percentage\n",
    "countries_with_labels_total = labels_not_null[['labels_tags', 'countries_tags']]\n",
    "#print(countries_with_labels_total)\n",
    "\n",
    "# We group by the country and take the count of the number of organic labeled product per country\n",
    "countries_with_labels_organic = countries_with_labels_organic.groupby('countries_tags').count().sort_values(by='labels_tags', ascending=False)\n",
    "\n",
    "# We group by the country and take the count of the number of labeled product per country\n",
    "countries_with_labels_total = countries_with_labels_total.groupby('countries_tags').count().sort_values(by='labels_tags', ascending=False)\n",
    "\n",
    "# Filter the countries that have less than 10 products\n",
    "countries_with_labels_organic = countries_with_labels_organic[countries_with_labels_total.labels_tags >= 10]\n",
    "countries_with_labels_total = countries_with_labels_total[countries_with_labels_total.labels_tags >= 0]\n",
    "\n",
    "print(countries_with_labels_total)\n",
    "print(countries_with_labels_organic)\n",
    "\n",
    "countries_with_labels_organic_over_total = countries_with_labels_organic/countries_with_labels_total\n",
    "countries_with_labels_organic_over_total.sort_values(by = \"labels_tags\", ascending = False, inplace = True)\n",
    "countries_with_labels_organic_over_total.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "countries_with_labels_organic_over_total['countries_tags'] = countries_with_labels_organic_over_total.index\n",
    "countries_with_labels_organic_over_total.dropna(inplace = True)\n",
    "#print(countries_with_labels_organic_over_total)\n",
    "\n",
    "\n",
    "# WE SHOULD ALSO USE CATEGORIES TAGS, THEN DO A PERCENTAGE AND VISUALIZE ON A MAP IF WE HAVE ENOUGH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_geo = 'world-countries-curated.json'\n",
    "M = folium.Map(location=[100, 0], zoom_start=1)\n",
    "# choropleth maps bind Pandas Data Frames and json geometries.\n",
    "#This allows us to quickly visualize data combinations\n",
    "M.choropleth(geo_data=country_geo, data=countries_with_labels_organic_over_total,\n",
    "             columns=['countries_tags', 'labels_tags'],\n",
    "             key_on='feature.properties.name',\n",
    "             fill_color='YlGnBu', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name=\"Percentage of organic food per country\")\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average nutrition score per country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data[~current_data['nutrition-score-uk_100g'].isna()].groupby('countries_tags')['nutrition-score-uk_100g'].mean().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL QUESTION: What's on my plate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to answer the following: given the location where we live, where does my food most likely come from? Is it eco-friendly? Is it healthy (from the nutrition grade)? How likely is it organic? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
